{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70393b70beb14ba4",
   "metadata": {},
   "source": [
    "# Computer Vision WS-24/25 Assignment 2.4: Image Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de31dab643b62c1",
   "metadata": {},
   "source": [
    "## Part 0.1: Setting Up the Environment\n",
    "\n",
    "Before we tackle image classification, we need to setup our working environment.\n",
    "This includes a few lines of code to setup the jupyter environment and to verify our python environment.\n",
    "\n",
    "The cell below is optional, but makes for a more seamless debugging experience. The autoreload extension allows us to edit, and re-import our source files without having to restart the jupyter notebook kernel after every change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f5605d9fda773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:18.826467Z",
     "start_time": "2024-11-18T10:00:18.815047Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5079ebec1c24c5e",
   "metadata": {},
   "source": [
    "## Part 0.2: Google Colab Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699a78f2012842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:19.517250Z",
     "start_time": "2024-11-18T10:00:19.507511Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    # We are not running in Google Colab\n",
    "    # We assume the data is in the same directory as the notebook\n",
    "    FILE_PATH = 'data'\n",
    "else:\n",
    "    # We are running in Google Colab\n",
    "    # Fill in the path to the directory where you uploaded the notebooks and data\n",
    "    # For example if you have created a folder with the name 'CV2025' in your Google Drive and have stored the notebook there, set GOOGLE_DRIVE_PATH = 'CV2025'\n",
    "    # GOOGLE_DRIVE_PATH = \"CV2025\"\n",
    "    GOOGLE_DRIVE_PATH = None\n",
    "    FILE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH)\n",
    "    os.chdir(FILE_PATH)\n",
    "    FILE_PATH = os.path.join(FILE_PATH, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f240be3317a66",
   "metadata": {},
   "source": [
    "# Part 0.3: Relevant python modules\n",
    "\n",
    "In the cell below we include all the static python libraries that we will use during this assignment. If you feel like importing additional libraries for visualization or debugging purposes, please feel free to add them here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41feb5f32b6a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:20.783126Z",
     "start_time": "2024-11-18T10:00:20.372180Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tqdm.contrib.itertools import product\n",
    "from rand import reset_seeds\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16  \n",
    "\n",
    "###################################################\n",
    "######## Add your preferred libraries here ########\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ef6540032995a",
   "metadata": {},
   "source": [
    "To optimize the usage of Colab (or your own local environment), we prefer the usage of GPU acceleration when working with PyTorch. The below cell will check for availability of any GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adee5c3a7a3b739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:21.395598Z",
     "start_time": "2024-11-18T10:00:21.252535Z"
    }
   },
   "outputs": [],
   "source": [
    "has_GPU = torch.cuda.is_available()\n",
    "\n",
    "if has_GPU:\n",
    "    print(\"GPU device found! We are good to go.\")\n",
    "else:\n",
    "    print(\"No GPU found. Please set the accelerator in Colab via Notebook Settings.\\nOtherwise we use CPU acceleration.\")\n",
    "\n",
    "DEVICE = 'cuda' if has_GPU else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49143b53321f07b",
   "metadata": {},
   "source": [
    "# Part 0.4: Getting Started: Data Preparation\n",
    "\n",
    "For the purpose of this assignment we will be dealing with CIFAR10, a 10 class image classification problem with $32\\times32$ RGB images. \n",
    "\n",
    "Below, we initialize our Dataset class anc call a function to preprocess the dataset.\n",
    "\n",
    "The utility function `dataset.get_splits(x)` takes our desired device as argument and returns 6 **torch.Tensor** instances:\n",
    "\n",
    "- `X_train`: all training images flattened into a 1D vector (normalized to have $\\mu=0$ and $\\sigma=1$)\n",
    "- `y_train`: the corresponding training labels (integers in $[0,9]$)\n",
    "- `X_val`: all validation images flattened into a 1D vector (normalized with the $\\mu_{\\text{train}}, \\sigma_{\\text{train}}$)\n",
    "- `y_val`: the corresponding validation labels\n",
    "- `X_test`: all testing images flattened into a 1D vector (normalized with the $\\mu_{\\text{train}}, \\sigma_{\\text{train}}$)\n",
    "- `y_test`: the corresponding testing labels\n",
    "\n",
    "*Note*: For the purpose of linear classifiers, we pad each image with an additional $1$ to include the bias term implicitly in the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f8977d8d58686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:27.121852Z",
     "start_time": "2024-11-18T10:00:23.967817Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import CIFAR10Dataset\n",
    "# Load the data\n",
    "reset_seeds()\n",
    "dataset = CIFAR10Dataset(data_dir=FILE_PATH)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = dataset.get_splits(DEVICE, include_bias=True, trainval_split=0.8, X_dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(f\"Train dataset containing {X_train.shape[0]} images\")\n",
    "print(f\"Validation dataset containing {X_val.shape[0]} images\")\n",
    "print(f\"Test dataset containing {X_test.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e41aeb41cdad5",
   "metadata": {},
   "source": [
    "Our dataset class also has a neat helper function that lets us visualize a few samples of a given dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76cb471d38838c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:27.350884Z",
     "start_time": "2024-11-18T10:00:27.122685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "reset_seeds()\n",
    "dataset.visualize_samples(X_train, y_train, to_show=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd23117",
   "metadata": {},
   "source": [
    "# Part 0.5: Optional: PyTorch basics\n",
    "\n",
    "In this notebook and in future assignments we will rely on the Pytorch library.\n",
    "For those of you who are not familiar with PyTorch we highly recommend to go over some of the basic tensor operations that PyTorch offers.\n",
    "\n",
    "There is a plethora of available material on the internet. Some introductions for tensor operations can be found here:\n",
    "- https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html\n",
    "- https://www.youtube.com/watch?v=x9JiIFvlUwk&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38da5b1e4cfaacb",
   "metadata": {},
   "source": [
    "# Task 1: Linear Models - The basics\n",
    "\n",
    "In this section of the assignment we will start our classification journey!\n",
    "\n",
    "We will start with the most basic form of linear classifiers, the **Linear SVM** and the **Softmax Classifier**.\n",
    "\n",
    "In the first part, you will warm up with the implementation of linear models and their basic functionalities.\n",
    "You will implement:\n",
    "\n",
    "- the **forward pass** of a linear model (0.5pts)\n",
    "- a **naive SVM loss function** (1.5pts)\n",
    "- a **fully vectorized version** of the above function (1.5pts)\n",
    "- a **fully vectorized version** of the Cross Entropy loss function (1.5+1pts)\n",
    "- the **inference** call to classify a given image (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345b6077f382ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:27.364531Z",
     "start_time": "2024-11-18T10:00:27.351564Z"
    }
   },
   "outputs": [],
   "source": [
    "# CIFAR10 contains 32 by 32 RGB images -> Our SVM receives a 3072 dimensional vector and outputs 10 logits for each class\n",
    "IMAGE_DIMS = 32**2 * 3\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cb94d3e82bcaf",
   "metadata": {},
   "source": [
    "## Part 1.1: Forward Pass (0.5pts)\n",
    "\n",
    "To start off this assignment, we start with the forward pass of our linear model.\n",
    "Please implement the forward pass of the `LinearBaseModel` in the file `models_factory`.\n",
    "For a batch of $N$ inputs $X\\in\\mathbb{R}^{N\\times D}$, the forward pass of our model computes a set of scores $S\\in\\mathbb{R}^{N\\times C}$ for each class $C$:\n",
    "\n",
    "$S=XW^T$.\n",
    "\n",
    "To verify the functionality of your implementation, we will run the below code block.\n",
    "(For reference, you should see an output score of $0.0061$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b53412e3cf0a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:28.876584Z",
     "start_time": "2024-11-18T10:00:28.795753Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_factory import LinearBaseModel\n",
    "\n",
    "reset_seeds()\n",
    "X_test = torch.randn(1, 1, IMAGE_DIMS+1, device=DEVICE)\n",
    "svm = LinearBaseModel(IMAGE_DIMS, 1, W_dtype=torch.float32, device=DEVICE)\n",
    "score = svm(X_test)\n",
    "print(f\"Score: {score.item(): .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888d9ad0d2b4b61",
   "metadata": {},
   "source": [
    "## Task 1.2: A naive SVM Loss (1.5pt)\n",
    "\n",
    "Please implement the naive SVM loss function in the function `svm_loss_naive` in file `model_factory.py`. What we refer to as naive is a traditional implementation where batch samples and class-scores are dealt with in a looped manner. We will allow all the syntactic sugar that PyTorch offers in the following task.\n",
    "\n",
    "To verify your implementation, you can run the below code block where we compare your implementation with the **PyTorch** implementation of the SVM loss.\n",
    "\n",
    "*Note*: PyTorch averages the loss over the number of possible classes. For the purpose of this assignment, we also use this additional averaging as it leads to a more numerically stable optimization in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70135090569f9e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T10:00:46.657037Z",
     "start_time": "2024-11-18T10:00:46.564316Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_factory import SVM\n",
    "\n",
    "reset_seeds()\n",
    "svm = SVM(IMAGE_DIMS, N_CLASSES, W_dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# Sample a small subset of the training data into a training batch\n",
    "X_batch = X_train[:64]\n",
    "y_batch = y_train[:64]\n",
    "\n",
    "\n",
    "# The forward pass of our svm will return the loss, as well as the gradient wrt our weight matrix W\n",
    "scores, loss = svm.loss(X_batch, y_batch, mode='naive', return_scores=True)\n",
    "\n",
    "pytorch_loss = F.multi_margin_loss(scores, y_batch)\n",
    "\n",
    "# Lets verify if the calulated loss is correct!\n",
    "try:\n",
    "    assert torch.allclose(loss, pytorch_loss)\n",
    "except AssertionError:\n",
    "    print(\"Looks like the Loss is wrong!\")\n",
    "else:\n",
    "    print(\"Looks good! The Naive SVM loss is implemented correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c6681b33fcec1",
   "metadata": {},
   "source": [
    "## Task 1.3: Vectorization (1.5pt)\n",
    "\n",
    "Now its time to implement the vectorized version of the same loss! Please implement the function `svm_loss_vectorized` in the file `model_factory.py` and run the below cell to verify its functionality. This function should contain **no loops**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a1bff96ef882a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:08.022475Z",
     "start_time": "2024-11-16T12:55:08.007656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample a small subset of the training data into a training batch\n",
    "X_batch = X_train[:64]\n",
    "y_batch = y_train[:64]\n",
    "\n",
    "\n",
    "# The forward pass of our svm will return the loss, as well as the gradient wrt our weight matrix W\n",
    "scores, loss = svm.loss(X_batch, y_batch, mode='vectorized', return_scores=True)\n",
    "\n",
    "pytorch_loss = F.multi_margin_loss(scores, y_batch)\n",
    "\n",
    "# Lets verify if the calulated loss is correct!\n",
    "try:\n",
    "    assert torch.allclose(loss, pytorch_loss)\n",
    "except AssertionError:\n",
    "    print(\"Looks like the Loss is wrong!\")\n",
    "else:\n",
    "    print(\"Looks good! The vectorized SVM loss is implemented correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0c95a88821aa0",
   "metadata": {},
   "source": [
    "By implementing the losses in vectorized form, we can use **PyTorch**'s optimized functions on the GPU to observe a speedup of anywhere between $15$ to $300$ times depending on the used hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b1bc7a44a51c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:08.080252Z",
     "start_time": "2024-11-16T12:55:08.023601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample a small subset of the training data into a training batch\n",
    "X_batch = X_train[:64]\n",
    "y_batch = y_train[:64]\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "loss_naive = svm.loss(X_batch, y_batch, mode='naive', return_scores=False)\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "naive_runtime = 1000. * (end - start)\n",
    "print(f\"Naive forward computed in{naive_runtime: .3f} ms\")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "# The forward pass of our svm will return the loss, as well as the gradient wrt our weight matrix W\n",
    "loss_vec = svm.loss(X_batch, y_batch, mode='vectorized', return_scores=False)\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "vectorized_runtime = 1000. * (end - start)\n",
    "print(f\"Vectorized forward computed in{vectorized_runtime: .3f} ms\")\n",
    "\n",
    "\n",
    "print(f\"Speedup Factor: {naive_runtime / vectorized_runtime:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5cd3c5a4c0228",
   "metadata": {},
   "source": [
    "## Part 1.4: Softmax Classifier (1.5+1pts)\n",
    "\n",
    "Another type of linear classifier is the **Softmax Classifier**.\n",
    "\n",
    "The Softmax classifier inherits its name from the use of the **Softmax function** to squish output logits into what can be interpreted as a probability distribution (note: technically it is not).\n",
    "\n",
    "The Softmax function is defined as follows:\n",
    "$\\sigma(\\mathbf{z})_i=\\frac{e^{s_i}}{\\sum_{j=1}^{C}e^{s_j}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c6622eb0ac3fb",
   "metadata": {},
   "source": [
    "### Part 1.4.1: A vectorized softmax / Cross Entropy Loss (1.5pt)\n",
    "\n",
    "A more conventional name for the loss function of the Softmax classifier is the **Cross Entropy loss**.\n",
    "Please implement the function `cross_entropy_loss` in the file `model_factory.py`. As before, this function should contain **no loops**!\n",
    "\n",
    "To verify that your implementation is correct, we will compare it with the **PyTorch** implementation of the Cross Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd59c489fd7251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:08.095395Z",
     "start_time": "2024-11-16T12:55:08.080834Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_factory import SoftmaxClassifier\n",
    "reset_seeds()\n",
    "\n",
    "smc = SoftmaxClassifier(IMAGE_DIMS, N_CLASSES, W_dtype=torch.float32, device=DEVICE)\n",
    "# Sample a small subset of the training data into a training batch\n",
    "X_batch = X_train[:64]\n",
    "y_batch = y_train[:64]\n",
    "\n",
    "\n",
    "# The forward pass of our svm will return the loss, as well as the gradient wrt our weight matrix W\n",
    "scores, loss = smc.loss(X_batch, y_batch, mode='vectorized', return_scores=True)\n",
    "\n",
    "pytorch_loss = F.cross_entropy(scores, y_batch)\n",
    "# Lets verify if the calulated loss is correct!\n",
    "try:\n",
    "    assert torch.allclose(loss, pytorch_loss)\n",
    "except AssertionError:\n",
    "    print(\"Looks like the Loss is wrong!\")\n",
    "else:\n",
    "    print(\"Looks good! The vectorized Cross Entropy loss is implemented correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0826b47188f684",
   "metadata": {},
   "source": [
    "### Part 1.4.2: Numerical Stability (1pt)\n",
    "\n",
    "The softmax function can suffer from numerical instabilities depending on its implementation.\n",
    "\n",
    "Lets check, if your implementation is numerically stable.\n",
    "If it is not, can you think of some tricks to make it stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f505294863debe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:08.109679Z",
     "start_time": "2024-11-16T12:55:08.095961Z"
    }
   },
   "outputs": [],
   "source": [
    "smc.W = torch.nn.Parameter(torch.ones_like(smc.W))\n",
    "\n",
    "loss_vec = smc.loss(X_batch, y_batch, mode='vectorized', return_scores=False)\n",
    "\n",
    "try:\n",
    "    assert not torch.isnan(loss_vec)\n",
    "except AssertionError:\n",
    "    print(\"Cross Entropy loss is not numerically stable!\")\n",
    "else:\n",
    "    print(\"Cross Entropy loss is numerically stable!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11692bb40d0f6d2a",
   "metadata": {},
   "source": [
    "## Part 1.5: Inference (1pt)\n",
    "\n",
    "Now that we have implemented the forward pass and the loss functions, we can use our models to classify images!\n",
    "For that, we need to implement the function `predict` in the file `model_factory.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758393bf9d5a23e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:08.126775Z",
     "start_time": "2024-11-16T12:55:08.110381Z"
    }
   },
   "outputs": [],
   "source": [
    "smc = SoftmaxClassifier(IMAGE_DIMS, N_CLASSES, W_dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "\n",
    "y_train_predict = svm.predict(X_train)\n",
    "train_acc = 100. * (y_train == y_train_predict).mean(dtype=torch.float64).item()\n",
    "\n",
    "y_val_predict = svm.predict(X_val)\n",
    "val_acc = 100. * (y_val == y_val_predict).mean(dtype=torch.float64).item()\n",
    "\n",
    "print(f\"Training accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Validation accuracy: {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff7ed220f77c17",
   "metadata": {},
   "source": [
    "# Part 2: Training the Linear Model\n",
    "\n",
    "After implementing all the building stones, we can now start optimizing the poor performance of our linear models.\n",
    "\n",
    "In this section, we will implement the training procedure for our linear models by optimizing them with **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "You will implement\n",
    "- creating **batches of training data** to feed to our models (1pt)\n",
    "- the training loop to optimize a linear model with **Stochastic Gradient Descent (SGD)** and $L_2$ **regularization** (1pt)\n",
    "- a **hyperparameter sweep** to figure out good parameters for **learning rate AND regularization strength** (1pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e3d35997fd3a4",
   "metadata": {},
   "source": [
    "## Part 2.1: Batching (1pt)\n",
    "\n",
    "In practice, it is infeasible to run a full gradient descent optimization on the entire dataset.\n",
    "Instead, we split the dataset into smaller batches and optimize our model on these batches, this is called **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "Please implement the function `get_batches` in the file `model_factory.py` to split the dataset into smaller batches of a certain size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790df32373c27386",
   "metadata": {},
   "source": [
    "## Part 2.2: Training Loop (1pt)\n",
    "\n",
    "Now that we have implemented the batching, we can start training our linear models!\n",
    "\n",
    "In the function `train_loop` in the file `model_factory.py`, you will implement the training loop for our linear models.\n",
    "The training loop should run for a certain number of iterations and optimize the model with **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "You can run the below cell to verify the functionality of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d050e9554341dd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:09.390097Z",
     "start_time": "2024-11-16T12:55:08.127397Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_seeds()\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "loss_history = svm.train_loop(X_train, y_train, lr=3e-9, reg=2.5e2, n_iter=1500, batch_size=200, verbose=True)\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Train loop has finished in {end-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4b7f2d683b439",
   "metadata": {},
   "source": [
    "To figure out what is happening to our model, it makes sense to look at the loss values to see if our model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6685c8a45dfac1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:55:09.468432Z",
     "start_time": "2024-11-16T12:55:09.390887Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylim(0, max(loss_history)+1)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152bcfc30178fc",
   "metadata": {},
   "source": [
    "## Part 2.3: Hyperparameter Sweep (1pt)\n",
    "\n",
    "Our current model has quite poor performance! We have also already seen the reason for this in the stagnating loss that we plotted above.\n",
    "\n",
    "To solve this issue, it is time to determine better hyperparameters with a **hyperparameter sweep**.\n",
    "\n",
    "Below, we have set up a grid-search for the optimal learning rate and regularization strength.\n",
    "\n",
    "Using the validation set, determine the optimal set of hyperparameters for our model and train it with these parameters.\n",
    "\n",
    "Please do this step for **both** the **Softmax Classifier** and the **SVM**.\n",
    "\n",
    "Also, please feel free to experiment with different values in the grid search! The current values are just a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bd80c8564838e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:02:49.606535Z",
     "start_time": "2024-11-16T12:59:58.395962Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_seeds()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = dataset.get_splits(DEVICE,include_bias=True, trainval_split=0.8, X_dtype=torch.float32)\n",
    "\n",
    "results = {}\n",
    "best_acc = -1\n",
    "best_model = None\n",
    "n_iter = 1500\n",
    "iter_idx = 0\n",
    "learning_rates = [1e-8 * (2.)**i for i in range(1,25)]\n",
    "reg_strengths = [1e-7 * (6.)**i for i in range(0,15)]\n",
    "num_models = len(learning_rates) * len(reg_strengths)\n",
    "\n",
    "# model_type = SoftmaxClassifier\n",
    "model_type = SVM\n",
    "\n",
    "for i, (lr, reg) in enumerate(product(learning_rates, reg_strengths, desc=f\"Training {num_models } Classifiers...\")):\n",
    "    model = model_type(IMAGE_DIMS, N_CLASSES, device=DEVICE, W_dtype=torch.float32)\n",
    "    model.train_loop(X_train, y_train, lr=lr, reg=reg, n_iter=n_iter, batch_size=200, verbose=False)\n",
    "\n",
    "    results[(lr, reg)] = None\n",
    "    \"# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\"\n",
    "    pass\n",
    "    \"# *****End OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\"\n",
    "    \n",
    "print(f\"Best validation accuracy achieved during grid search: {best_acc}\\n with the {str(best_model)}\")\n",
    "\n",
    "# Print out results into a file for debugging.\n",
    "with open(f\"{str(best_model)}_grid_search_losses.txt\", \"w\") as f:\n",
    "    for lr, reg in sorted(results):\n",
    "        val_acc = results[(lr, reg)]\n",
    "        f.write(f\"lr {lr} reg {reg} val accuracy: {val_acc:.2f}\\n\")\n",
    "best_model.save(f\"best_{str(best_model)}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea472191024839b",
   "metadata": {},
   "source": [
    "To visualize the results of the grid search, we can plot the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b026a6b0b3c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:58:44.981781Z",
     "start_time": "2024-11-16T12:58:44.874157Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "\n",
    "marker_size = 100\n",
    "# plot validation accuracy\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a36d38ae9cacc",
   "metadata": {},
   "source": [
    "And finally, we evaluate our best model. To get full credit for the assignment, your best models should both have an accuracy above  35%.\n",
    "(As a reference, our best model achieved $\\approx 40.88% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f7505ea7639bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:02:55.052459Z",
     "start_time": "2024-11-16T13:02:55.036034Z"
    }
   },
   "outputs": [],
   "source": [
    "best_svm = SVM(IMAGE_DIMS, N_CLASSES, W_dtype=torch.float32, device=DEVICE)\n",
    "best_svm.load(f\"best_{str(best_svm)}.pth\")\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = 100. * torch.mean((y_test == y_test_pred).double())\n",
    "print(f\"SVM final test set accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "best_smc = SoftmaxClassifier(IMAGE_DIMS, N_CLASSES, W_dtype=torch.float32, device=DEVICE)\n",
    "best_smc.load(f\"best_{str(best_smc)}.pth\")\n",
    "y_test_pred = best_smc.predict(X_test)\n",
    "test_accuracy = 100. * torch.mean((y_test == y_test_pred).double())\n",
    "print(f\"SoftmaxClassifier final test set accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73847b143f4cc7b1",
   "metadata": {},
   "source": [
    "# (Bonus) Part 3: Beyond Linear Classification\n",
    "\n",
    "Learning a linear model for classification is a good learning exercise.\n",
    "But images are not highly non-linear functions and to learn the intricate semantics of images to classify them reliably linear models are not sufficient.\n",
    "\n",
    "We have seen that our linear models have a hard time classifying the CIFAR10 dataset. In this bonus exercise you will learn to go **beyond** the limitations of linearity.\n",
    "\n",
    "In this bonus task you can experiment with two architectures:\n",
    "- **Multi Layer Perceptron (MLP)** (1+0.5+0.5pts)\n",
    "  - implement a MLP architecture including its forward pass (1pt)\n",
    "  - train it with the previously implemented **Hinge Loss** (0.5pts)\n",
    "  - train it with the previously implemented **Cross Entropy Loss** (0.5pts)\n",
    "- **Convolutional Neural Network (CNN)** (1+0.5+0.5pts)\n",
    "    - implement a CNN architecture with a classification head including its forward pass (1pt)\n",
    "    - train it with the previously implemented **Hinge Loss** (0.5pts)\n",
    "    - train it with the previously implemented **Cross Entropy Loss** (0.5pts)\n",
    "\n",
    "For this bonus part of the assignment, we will give you a lot of freedom. You are allowed to use as many of the previously implemented functions as you like. \n",
    "The grading will be done based on the performance of your models on the test set and their implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27795d12f1b134",
   "metadata": {},
   "source": [
    "#### Get Creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971834790ec63d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
